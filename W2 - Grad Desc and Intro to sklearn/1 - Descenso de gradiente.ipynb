{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje estadístico y optimización matemágica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De aprendizaje automático a optimización matemática\n",
    "\n",
    "Los algoritmos de **machine learning** de tipo **supervisado** son aquellos que estiman una **función $\\hat{f}$ de mapeo** entre las variables de entrada y una variable de salida. \n",
    "\n",
    "Recordemos que esta función $\\hat{f}$ es una función que no tiene por qué ser la función real si no una aproximación que nos de unos **resultados de predicción aceptables** para el caso de uso sobre el que estemos entrenando el algoritmo.\n",
    "\n",
    "Si nos regimos bajo ese paradigma, podemos entender entonces que **dado un modelo inicial en función de unos parametros**, podemos simplificar **el proceso de aprendizaje** a encontrar esos **parametros que nos minimizan el error** que cometemos al comparar las predicciones con el resultado esperado.\n",
    "\n",
    "Pasamos entonces de un problema de aprendizaje estadístico a un problema de **optimización matemática** de la expresión del error que cometemos en las predicciones de forma que se minimize el error global sobre todo el dataset.\n",
    "\n",
    "\n",
    "## Función de perdida y de coste\n",
    "\n",
    "Con tal de poder resolver el problema de optimización para minimizar cuanto nos equivocamos necesitamos una expresión matemática sobre la que podamos operar. A la función de error para una instancia de entrenamiento, es decir, a la que aplica a un sola dupla **características - objetivo**, o sea ($X - y$), la llamaremos función de perdida (en inglés, *loss function*). Por otro lado, cuando **agreguemos** esa función a nivel de todo el dataset de entrenamiento, la llamaremos función de coste (en inglés, **cost function**).\n",
    "\n",
    "> **Función de perdida**  $L(\\hat{f}(\\omega), y)$\n",
    "> \n",
    "> Error de una instancia de entrenamiento\n",
    ">\n",
    "\n",
    "\n",
    "\n",
    "> **Función de Coste** $J(\\omega)$\n",
    ">\n",
    "> Error medio para todo el dataset de entreno. Se calcula agregando la funcion de perdida, por ejemplo la media de los errores sobre todo el dataset\n",
    "\n",
    "Esta función puede tomar varias formas y sobre todo dependerá de la tarea a realizar, si es regresión o clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de la función de coste\n",
    "\n",
    "Una vez determinada una función de coste en función de los parametros de nuestro algoritmo, la optimización de esta función nos dara los parámetros de nuestro modelo\n",
    "que nos reducen el error de predicción.\n",
    "\n",
    "Si bien dependiendo de la función de coste la optimización se puede hacer de forma analítica, una de las técnicas más extendidas de optimización en el campo del aprendizaje automático es el **descenso de gradiente**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descenso de gradiente\n",
    "\n",
    "El **gradiente de una función** puede ser interpretado como el vector que nos indica la dirección y ratio con el que la función aumenta en un determinado punto.\n",
    "\n",
    "Sabemos que en los mínimos y máximos, tanto locales como globales, de una función el gradiente será igual a cero. El **descenso de gradiente** consiste precisamente en acercarse de forma iterativa a uno de esos mínimos moviendonos en la dirección contraria a la que nos indica el gradiente en un punto para unos parámetros dados.\n",
    "\n",
    "![gradient](./img/gradient.jpeg)\n",
    "\n",
    "Matemáticamente, esto se puede escribir cómo:\n",
    "\n",
    "$$\\omega_{i+1} = \\omega_{i} - \\alpha \\times \\frac{\\partial J(\\omega_i)}{\\partial \\omega}$$\n",
    "\n",
    "> Atención al signo negativo: siempre querremos movernos en la dirección opuesta al gradiente.\n",
    "\n",
    "Si iteramos sobre la expresión anterior a partir de unos pesos $\\omega$ inicializados aleatoriamente, nos acercaremos al mínimo en cada paso de manera proporcional al gradiente, que irá disminuyendo a medida que nos acerquemos al mínimo, y al hiperparametro $\\alpha$. Este hiperparametro permite ajustar la velocidad a la que el descenso de gradiente se acerca al mínimo global y será una de las principales palancas de acción sobre el entrenamiento sobre la que podremos jugar para poder encontrar el mínimo de la función de coste.\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "El descenso de gradiente es una técnica muy presente en el campo del aprendizaje automático, especialmente en el aprendizaje profundo. Esta técnica nos permite optimizar la función de coste del algoritmo de forma que encontremos los parametros que minimizan el error en las predicciones. En resumen, la dinámica se resume en:\n",
    "\n",
    "\n",
    "1. Encontrar la función de coste del algoritmo\n",
    "\n",
    "2. Encontrar la el gradiente de la función de coste\n",
    "\n",
    "3. Aplicar el descenso de gradiente evaluando el dataset de entrenamiento en el gradiente de la función de coste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso práctico: Regresión lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la aplicación del descenso de gradiente sobre una regresión lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función real: las observaciones son fruto de una función desconocida $f$ más un error $\\epsilon$\n",
    "\n",
    "$$ y = f(x) + \\epsilon $$\n",
    "\n",
    "**Hipótesis:**\n",
    "\n",
    "Función a estimar f(x) es lineal, planteamos entonces:\n",
    "\n",
    "$$ \\hat{y} = \\hat{f}(x) = \\theta_0 + \\theta_1 x \\qquad (1)$$   \n",
    "\n",
    "## Función de coste\n",
    "\n",
    "Con la relación (1), teniendo los valores de la variable dependiente x y su observacion y para varios registros, podemos plantear las función de error y de coste cómo:\n",
    "\n",
    "\n",
    "$$ L_i(\\theta_0, \\theta_1) = (\\hat{f}(x_i) - y_i)^2 = ((\\theta_0 + \\theta_1 x_i) - y_i)^2 $$\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum^m_i L_i(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum^m_i ((\\theta_0 + \\theta_1 x) - y_i)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:23.993097Z",
     "start_time": "2018-08-25T03:56:23.138209Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.006246Z",
     "start_time": "2018-08-25T03:56:23.998204Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1 - Dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar un dataset sintetico de forma que siga una relacion lineal:\n",
    "\n",
    "$$y = \\theta_0 + \\theta_1 x $$ \n",
    "\n",
    "Esta será la función que querremos aproximar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declara las variables theta_0 y theta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = 4\n",
    "theta_1 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Genera un dataset aleatorio\n",
    "\n",
    "## Establece la semilla\n",
    "np.random.seed(0)\n",
    "\n",
    "## declara el tamaño del dataset\n",
    "size = (200, 1)\n",
    "\n",
    "X_1 = np.random.normal(0, 1, size=size)\n",
    "X_0 = np.ones_like(X_1)\n",
    "\n",
    "error = np.random.normal(0, 1, size=size)\n",
    "\n",
    "# Genera la variable objetivo y en función a la eq lineal\n",
    "y = theta_0 * X_0 + theta_1*X_1 + error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a graficar nuestros datos para comprobar la relacion de y ~ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.780299Z",
     "start_time": "2018-08-25T03:56:24.032204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.scatterplot(y=y[:,0], x=X_1[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2 - Solución analítica\n",
    "\n",
    "Dado que la función de coste puede resolverse de forma analítica, vamos a resolverla para encontrar los valores de $\\theta$ y poder comparar con los valores que obtenemos con gradient descent\n",
    "\n",
    "Recordamos que la solución anlítica viene dada de forma matricial como:\n",
    "\n",
    "\n",
    "$$\\theta = [\\theta_0, \\theta_1] = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "[derivacion](https://towardsdatascience.com/analytical-solution-of-linear-regression-a0e870b038d5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Juntamos los dos vectores en una matrix. Recuerda que X_0 = [1, ...,1] ya que multiplica a la theta_0 (ordenada en el origen)\n",
    "\n",
    "X = np.hstack([X_0, X_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos la transpuesta\n",
    "\n",
    "X_t = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculamos el resultado del producto de la transpuesta y multiplicamos por la transpuesta otra vez\n",
    "\n",
    "XtX = X_t.dot(X)\n",
    "XtX_inv = np.linalg.inv(XtX)\n",
    "XtX_invXt = ## usa el metodo .dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.792752Z",
     "start_time": "2018-08-25T03:56:24.783380Z"
    }
   },
   "outputs": [],
   "source": [
    "## Finalmente, multiplicamos por las y's conocidas\n",
    "theta_est = XtX_invXt.dot(y)\n",
    "\n",
    "print(theta_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Finalmente, vamos a plottear el resultado para comparar la linea real con la estimada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:25.168502Z",
     "start_time": "2018-08-25T03:56:24.808124Z"
    }
   },
   "outputs": [],
   "source": [
    "## Graficamos los datos de entrenamiento\n",
    "\n",
    "ax = sns.scatterplot(y=y[:,0], x=X_1[:,0], label='Real')\n",
    "\n",
    "## Creamos una y estimada a partir de los datos de X y de theta_est\n",
    "y_hat = ##\n",
    "\n",
    "sns.scatterplot(y=y_hat[:, 0], x=X[:, 1], label='Estimado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular el error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = ##\n",
    "sum_squared_error = ## producto escalar del vector error sobre si mismo\n",
    "mean_squared_error = sum_squared_error / error.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los valores obtenidos son aproximados a los reales que hemos impuesto a principio del ejercicio.\n",
    "\n",
    "Al ser la solución analítica, es la solución óptima a la que querremos acercarnos via el descenso de gradiente.\n",
    "\n",
    "Destacar que **nunca** podremos llegar a una estimación perfecta de los parametros reales debido a la presencia de ruido en nuestras mediciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra la derivación de la función de coste sobre la cual aplicaremos el método de descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de coste**\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum^m_i L_i(\\theta_0, \\theta_1) = \\frac{1}{2m}\\sum^m_i ((\\theta_0 + \\theta_1 x) - y_i)^2 $$\n",
    "\n",
    "**Gradientes**\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_0} = \\frac{1}{m}\\sum_{i=1}^{m}((\\theta_0 + \\theta_1 x) - y_i)) $$\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_1} = \\frac{1}{m}\\sum_{i=1}^{m}((\\theta_0 + \\theta_1 x_i) - y_i))x_i $$\n",
    "\n",
    "**Descenso de gradiente**\n",
    "\n",
    "$$\\theta_0^{t+1} := \\theta_0^t - \\alpha \\times \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_0}$$\n",
    "\n",
    "$$\\theta_1^{t+1} := \\theta_1^t - \\alpha \\times \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex-1: Crea una función que calcule el coste de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_j(theta_0: float, theta_1: float, X: np.array, y: np.array) -> float:\n",
    "    \"\"\"Calcula el error cuadrático medio\n",
    "    \"\"\"\n",
    "    m = ##\n",
    "    y_pred = ##\n",
    "    coste = ##\n",
    "    coste_quad = coste.T.dot(coste) ## Esto se puede escribir como np.sum(np.square(y_pred - y))\n",
    "    coste_quad_medio = ##\n",
    "    return float(coste_quad_medio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex-2: Crea una función que devuelva una tupla con los gradientes parciales para $\\theta_0$ y $\\theta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(theta_0: float, theta_1: float, X: np.array, y: np.array) -> float:\n",
    "    m = #\n",
    "    y_pred = theta_0 + theta_1*X\n",
    "    z = y_pred - y\n",
    "    grad_theta_0 = ##\n",
    "    grad_theta_1 = float(z.T.dot(X)/m)\n",
    "    return grad_theta_0, grad_theta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradiente(1000,600, X_1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex-3: Inicializa aleatoriamente $\\theta_0$ y $\\theta_1$ y aplica el algoritmo de descenso de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inicalizacion de parametros\n",
    "theta_0 = ##\n",
    "theta_1 = ##\n",
    "\n",
    "## Selección de hiperparametros\n",
    "\n",
    "alpha = ##\n",
    "num_iteraciones = ##\n",
    "\n",
    "training_report = []\n",
    "\n",
    "for it in range(num_iteraciones):\n",
    "    # calculamos el coste\n",
    "    FOR \n",
    "        coste = ##\n",
    "\n",
    "        # calculamos el gradiente\n",
    "        grad_theta_0, grad_theta_1 = ##\n",
    "\n",
    "        # Actualizamos los parametros theta\n",
    "        theta_0 = theta_0 - alpha*grad_theta_0\n",
    "        theta_1 = theta_1 - alpha*grad_theta_1\n",
    "\n",
    "        ## Añadir info al informe\n",
    "\n",
    "        # Crea un diccionario con la información\n",
    "        tmp_report = {'iteracion':it,\n",
    "                      'batch': i,\n",
    "                      'batch_size': batch_size,\n",
    "                      'theta_0': theta_0,\n",
    "                      'theta_1': theta_1,\n",
    "                      'coste': coste}\n",
    "\n",
    "        training_report.append(tmp_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_report = pd.DataFrame(training_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_report.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_report[['iteracion', 'coste']].plot(kind='line', x='iteracion', y='coste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios:\n",
    "\n",
    "1 - Encapsula la lógica en una funcion que tome como argumentos: `alpha, num_iteraciones, X, y` y realiza varios experimentos con distintos learning rates e iteraciones\n",
    "\n",
    "2 - La lógica anterior usa todos los datos disponibles para entrenar el modelo. Cómo se llama a este tipo de entrenamiento ?\n",
    "\n",
    "3 - Modifica la funcion en el ejercicio 1 de forma que se pueda especificar el tamaño del batch de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "def train_batch_gd(alpha, num_iteraciones, X, y, batch_size):\n",
    "    logic"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Gradient Descent-Python",
    "public": true
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Py37 (ds-uib)",
   "language": "python",
   "name": "ds-uib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "461.183px",
    "left": "846.167px",
    "right": "138.333px",
    "top": "127px",
    "width": "559.667px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
